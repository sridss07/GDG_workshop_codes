{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sridss07/GDG_workshop_codes/blob/main/GDG_DAY_2_GEN_AI_CODE(2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-lRoTpqJTeM"
      },
      "source": [
        "### Install Required Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgALz5M7I9dQ",
        "outputId": "59fda35b-bd35-4cb5-9c58-7d6d695a0065"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m600.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m405.1/405.1 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "! pip install -q --upgrade google-generativeai langchain-google-genai python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ZKjZMhvJYJ-"
      },
      "outputs": [],
      "source": [
        "#///// prompt: create a .env file in the workspace\n",
        "# https://makersuite.google.com/\n",
        "\n",
        "!echo -e 'GOOGLE_API_KEY=GOOGLE_API_KEY' > .env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOf2L7wtJxPt",
        "outputId": "aed2f774-da1e-4f12-bd56-69764c564d71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ".  ..  .config\t.env  sample_data\n"
          ]
        }
      ],
      "source": [
        "!ls -a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFLSyf1cJzIL",
        "outputId": "9782ba89-1b26-48e1-ad24-1e3127e469d4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELCq2M4XJ467"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display\n",
        "from IPython.display import Markdown\n",
        "import textwrap\n",
        "\n",
        "\n",
        "def to_markdown(text):\n",
        "  text = text.replace('•', '  *')\n",
        "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CHCjBuPHJ-JU"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1cb7x-g7KBBw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "genai.configure(api_key=os.environ.get(\"GOOGLE_API_KEY\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NOFOt74X2Gd"
      },
      "source": [
        "### Text Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KG_3KsGRKDYU",
        "outputId": "6b54844e-4950-4e7f-c2f4-ee97b59a6412"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "genai.GenerativeModel(\n",
              "    model_name='models/gemini-pro',\n",
              "    generation_config={},\n",
              "    safety_settings={},\n",
              "    tools=None,\n",
              "    system_instruction=None,\n",
              "    cached_content=None\n",
              ")"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = genai.GenerativeModel(model_name = \"gemini-pro\")\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-2GbIa-KaUb"
      },
      "outputs": [],
      "source": [
        "prompt = [\n",
        "    \"What is Mixture of Experts?\",\n",
        "]\n",
        "\n",
        "response = model.generate_content(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 584
        },
        "id": "i9TTvRUfKmBg",
        "outputId": "5dc1ff04-3bca-4892-a167-3f3959b7b99a"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "> **Mixture of Experts (MoE)**\n",
              "> \n",
              "> Mixture of Experts (MoE) is an ensemble learning technique that aims to improve the generalization ability of a model by combining the predictions of multiple specialized experts.\n",
              "> \n",
              "> **Concept:**\n",
              "> \n",
              "> * MoE models consist of a gating network and a set of expert networks.\n",
              "> * The gating network learns to assign different input instances to appropriate experts based on their expertise.\n",
              "> * Each expert network specializes in handling a specific subset of input data and makes predictions for its assigned instances.\n",
              "> \n",
              "> **Process:**\n",
              "> \n",
              "> 1. **Gated Input:** The input data is passed through the gating network, which generates a set of gating probabilities.\n",
              "> 2. **Expert Selection:** Each gating probability indicates the probability of an input instance belonging to a particular expert.\n",
              "> 3. **Expert Predictions:** The selected expert networks make predictions for their assigned instances.\n",
              "> 4. **Mixture Output:** The predictions from all experts are weighted and combined to form the final output.\n",
              "> \n",
              "> **Advantages:**\n",
              "> \n",
              "> * **Improved Generalization:** MoE models can generalize better by capturing the expertise of multiple individual experts.\n",
              "> * **Modular Architecture:** The expert networks can be trained independently, allowing for flexibility and scalability.\n",
              "> * **Specialization:** Experts can focus on specific aspects of the problem, leading to more accurate predictions.\n",
              "> \n",
              "> **Challenges:**\n",
              "> \n",
              "> * **Training Complexity:** MoE models can be computationally expensive to train due to the large number of expert networks.\n",
              "> * **Hyperparameter Tuning:** Optimizing the gating mechanism and expert networks requires careful hyperparameter tuning.\n",
              "> * **Interpretability:** Understanding the contributions of individual experts can be challenging.\n",
              "> \n",
              "> **Applications:**\n",
              "> \n",
              "> MoE models have been successfully applied to a wide range of tasks, including:\n",
              "> \n",
              "> * Image classification\n",
              "> * Natural language processing\n",
              "> * Speech recognition\n",
              "> * Time series forecasting"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "to_markdown(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V61OffK-4cBX",
        "outputId": "bf3d8564-8cad-4252-d82d-f71e73c6fa40"
      },
      "outputs": [
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# response.prompt_feedback\n",
        "# response.candidates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TbsjM7hK6PR"
      },
      "source": [
        "### Use LangChain to Access Gemini API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WqdODsdLK8li"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vknP_oOWK-k7"
      },
      "outputs": [],
      "source": [
        "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", google_api_key=\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hvY2YkFoLDO7"
      },
      "outputs": [],
      "source": [
        "result = llm.invoke(\"What is the sky blue ?\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "ar8KNtZILE47",
        "outputId": "b04647fa-4772-4900-a7d8-455c36faf3bb"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "> The sky appears blue due to a phenomenon called Rayleigh scattering. This refers to the scattering of light by particles that are smaller than the wavelength of light.\n",
              "> \n",
              "> In the case of the sky, the scattering particles are molecules of nitrogen and oxygen in the atmosphere. When sunlight passes through the atmosphere, the shorter wavelength blue light is scattered more than the longer wavelength red light. This is because the wavelength of blue light is closer to the size of the nitrogen and oxygen molecules than the wavelength of red light.\n",
              "> \n",
              "> The scattered blue light is then redirected in all directions, which is why we see the sky as blue from all angles. The amount of scattering depends on the wavelength of light and the number of scattering particles in the atmosphere."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "to_markdown(result.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5EiNEQ63PYu"
      },
      "source": [
        "# Chat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRCFsCD-3OsM",
        "outputId": "04ec40f5-93d6-4370-9f3e-7458d9737ca1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatSession(\n",
              "    model=genai.GenerativeModel(\n",
              "        model_name='models/gemini-1.5-flash',\n",
              "        generation_config={},\n",
              "        safety_settings={},\n",
              "        tools=None,\n",
              "        system_instruction=None,\n",
              "        cached_content=None\n",
              "    ),\n",
              "    history=[]\n",
              ")"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
        "chat = model.start_chat(history=[])\n",
        "chat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 62
        },
        "id": "pJMTMS3x3UJs",
        "outputId": "95091996-cab0-4bc1-b322-9ac2bd087394"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "> A computer is like a super smart toy that follows your instructions, showing you things on its screen and making sounds! \n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response = chat.send_message(\n",
        "    \"In one sentence, explain how a computer works to a young child.\"\n",
        ")\n",
        "to_markdown(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_iUvACre3XuE",
        "outputId": "2e630d0d-f094-48e8-c060-02b854f1f8b0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[parts {\n",
              "   text: \"In one sentence, explain how a computer works to a young child.\"\n",
              " }\n",
              " role: \"user\",\n",
              " parts {\n",
              "   text: \"A computer is like a super smart toy that follows your instructions, showing you things on its screen and making sounds! \\n\"\n",
              " }\n",
              " role: \"model\"]"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat.history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "t5PA2ibP3V38",
        "outputId": "59dff2c5-b90e-4204-d03c-208efbe498c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computers\n",
            "________________________________________________________________________________\n",
            " work by processing information, converting it into a series of ones and zeros (binary\n",
            "________________________________________________________________________________\n",
            " code), then using those instructions to display images, play sounds, and perform calculations\n",
            "________________________________________________________________________________\n",
            ", all at lightning speed. \n",
            "\n",
            "________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "response = chat.send_message(\n",
        "    \"Okay, how about a more detailed explanation to a high schooler?\", stream=True\n",
        ")\n",
        "\n",
        "for chunk in response:\n",
        "    print(chunk.text)\n",
        "    print(\"_\" * 80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "zN3KzNS33gv6",
        "outputId": "b4dac10e-141c-42d8-9c5f-a6b17584a7d1"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "> **user**: In one sentence, explain how a computer works to a young child."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "> **model**: A computer is like a super smart toy that follows your instructions, showing you things on its screen and making sounds! \n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "> **user**: Okay, how about a more detailed explanation to a high schooler?"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "> **model**: Computers work by processing information, converting it into a series of ones and zeros (binary code), then using those instructions to display images, play sounds, and perform calculations, all at lightning speed. \n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "for message in chat.history:\n",
        "    display(to_markdown(f\"**{message.role}**: {message.parts[0].text}\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjFmS23p4pwy"
      },
      "source": [
        "# Image & Text Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UW-OA-RO4Wnh",
        "outputId": "3f3cb46b-8b3e-4490-e5db-32f399163f94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  405k  100  405k    0     0  4906k      0 --:--:-- --:--:-- --:--:-- 4939k\n"
          ]
        }
      ],
      "source": [
        "!curl -o image.jpg https://t0.gstatic.com/licensed-image?q=tbn:ANd9GcQ_Kevbk21QBRy-PgB4kQpS79brbmmEG7m3VOTShAn4PecDU5H5UxrJxE3Dw1JiaG17V88QIol19-3TM2wCHw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 982
        },
        "id": "GH48QoWG4tmy",
        "outputId": "360bd3f5-dd11-429a-fa66-70951a1c1953"
      },
      "outputs": [],
      "source": [
        "import PIL.Image\n",
        "\n",
        "img = PIL.Image.open(\"image.jpg\")\n",
        "img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f17_jNea4tjG"
      },
      "outputs": [],
      "source": [
        "model = genai.GenerativeModel(\"gemini-1.5-flash\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "Kl-1kODp4tfi",
        "outputId": "34d92b2b-3806-421d-bf1e-dca144ac4fbb"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "> This is a delicious looking meal prep container with chicken, rice, broccoli, peppers and carrots. It looks like a healthy and satisfying meal!  The food is arranged neatly and the container is clear, allowing you to see the ingredients. The chopsticks and sesame seeds add a nice touch."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response = model.generate_content(img)\n",
        "\n",
        "to_markdown(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5gjLJmku46hd"
      },
      "outputs": [],
      "source": [
        "response = model.generate_content(\n",
        "    [\n",
        "        \"Write a short, engaging blog post based on this picture. It should include a description of the meal in the photo and talk about my journey meal prepping.\",\n",
        "        img,\n",
        "    ],\n",
        "    stream=True,\n",
        ")\n",
        "response.resolve()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "id": "GUaepDqz4_ga",
        "outputId": "4013364e-92ba-441b-d800-15ab7c52694c"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "> ## My Meal Prep Journey: From Chaos to Calm\n",
              "> \n",
              "> This picture? This is my life right now, in a nutshell. It's a glimpse into my journey of meal prepping, a journey that's transformed from utter chaos to a blissful haven of healthy, delicious meals.  \n",
              "> \n",
              "> The container in the foreground? That's my lunch for tomorrow. It's bursting with flavor: succulent teriyaki chicken, fluffy white rice, tender broccoli florets, and vibrant red peppers and carrots. It's a symphony of colors and textures, a testament to my newfound love of meal prepping. \n",
              "> \n",
              "> My journey began with a desire to eat healthier and more consistently. But the reality of my busy schedule made it nearly impossible to cook from scratch every night. Then came the revelation: meal prepping! \n",
              "> \n",
              "> I started small, prepping just one or two meals at a time. I learned by trial and error, experimenting with different recipes and techniques. And with each successful batch, my confidence grew. \n",
              "> \n",
              "> Now, meal prepping is my jam. I find joy in planning my meals, creating delicious recipes, and knowing that I have a healthy, homemade lunch waiting for me every day. It's made my life so much easier, and the satisfaction of knowing I'm taking care of myself is priceless. \n",
              "> \n",
              "> So, if you're on the fence about meal prepping, I say jump in! You might just be surprised at how much you enjoy it – and how much healthier your life becomes. \n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "to_markdown(response.text)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.9 (v3.10.9:1dd9be6584, Dec  6 2022, 14:37:36) [Clang 13.0.0 (clang-1300.0.29.30)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}